{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d6bc7e-2d12-4c27-af35-cccb06216e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, torch \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # to split the data for training, validation and testing\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5db3a0a-217a-41b9-adcf-a7715f32c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 video_id                                              path\n",
      "0    tadp_accident/video1    C:\\Users\\yashr\\Downloads\\sampled_frames\\video1\n",
      "1   tadp_accident/video10   C:\\Users\\yashr\\Downloads\\sampled_frames\\video10\n",
      "2  tadp_accident/video100  C:\\Users\\yashr\\Downloads\\sampled_frames\\video100\n",
      "3  tadp_accident/video101  C:\\Users\\yashr\\Downloads\\sampled_frames\\video101\n",
      "4  tadp_accident/video102  C:\\Users\\yashr\\Downloads\\sampled_frames\\video102\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Root folder where the sampled frames are\n",
    "frames_root = r\"C:\\Users\\yashr\\Downloads\\sampled_frames\"\n",
    "\n",
    "def convert_video_to_frame_path(video_path):\n",
    "    # Extracting parent folder\n",
    "    parent_folder = os.path.basename(os.path.dirname(video_path))\n",
    "    # Extracting video file name without extension\n",
    "    video_file = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    # Checking if video_file is of the format v<number> exactly\n",
    "    if re.fullmatch(r'v\\d+', video_file.lower()):\n",
    "        # Use parent-video combination\n",
    "        new_folder = f\"{parent_folder}-{video_file}\"\n",
    "    else:\n",
    "        new_folder = video_file\n",
    "    # Full path to sampled frames\n",
    "    new_path = os.path.join(frames_root, new_folder)\n",
    "    return os.path.normpath(new_path)\n",
    "\n",
    "df['path'] = df['path'].apply(convert_video_to_frame_path)\n",
    "\n",
    "# Saving updated CSV\n",
    "df.to_csv(\"updated_dataset.csv\", index=False)\n",
    "\n",
    "print(df[['video_id', 'path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20fcce2-444c-4fd1-ae9f-8c1179be1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = \"updated_dataset.csv\"\n",
    "OUT_DIR = \"splits\"\n",
    "\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "# splits the dataset into training set (70%) and remaining goes into temp set \n",
    "train, temp = train_test_split(\n",
    "    df, test_size=0.3, stratify=df[\"label\"], random_state=27\n",
    ")\n",
    "\n",
    "# splits the temp set into validation and test set equally (15% each)\n",
    "val, test = train_test_split(\n",
    "    temp, train_size=0.5, stratify=temp[\"label\"], random_state=27\n",
    ")\n",
    "\n",
    "train.to_csv(os.path.join(OUT_DIR, \"train.csv\"))\n",
    "val.to_csv(os.path.join(OUT_DIR, \"val.csv\"))\n",
    "test.to_csv(os.path.join(OUT_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa0b34-9370-4134-bd2a-6f536b149534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64415878.5625 64747539.0625 64213634.4375]\n",
      "[1.19920095e+08 1.23459132e+08 1.24473541e+08]\n",
      "[1.66625081e+08 1.71474641e+08 1.64899402e+08]\n",
      "[2.17187582e+08 2.22022605e+08 2.13919474e+08]\n",
      "[2.72964165e+08 2.77667982e+08 2.69406630e+08]\n",
      "[3.27863932e+08 3.32875120e+08 3.25818135e+08]\n",
      "[3.80567856e+08 3.87827299e+08 3.79308086e+08]\n",
      "[4.41138290e+08 4.49252994e+08 4.39862346e+08]\n",
      "[4.93270487e+08 5.02599764e+08 4.91752782e+08]\n",
      "[5.65614796e+08 5.75864767e+08 5.63923734e+08]\n",
      "[6.04631156e+08 6.15877681e+08 6.02454902e+08]\n",
      "[6.54380513e+08 6.69798744e+08 6.49545861e+08]\n",
      "[6.98758473e+08 7.19738008e+08 6.92428673e+08]\n",
      "[7.53049480e+08 7.72401436e+08 7.39588431e+08]\n",
      "[7.95786659e+08 8.14759167e+08 7.79660088e+08]\n",
      "[8.51277296e+08 8.70419190e+08 8.32721677e+08]\n",
      "[9.02136864e+08 9.22560154e+08 8.83854182e+08]\n",
      "[9.63284545e+08 9.81090748e+08 9.42842141e+08]\n",
      "[1.02591918e+09 1.04162412e+09 1.00401761e+09]\n",
      "[1.07184890e+09 1.08487794e+09 1.04740219e+09]\n",
      "[1.12701730e+09 1.14042268e+09 1.09930882e+09]\n",
      "[1.16843320e+09 1.18377677e+09 1.14168586e+09]\n",
      "[1.22004684e+09 1.23688394e+09 1.19326839e+09]\n",
      "[1.24274799e+09 1.26088981e+09 1.21633227e+09]\n",
      "[1.28779179e+09 1.30746771e+09 1.26185027e+09]\n",
      "[1.34182467e+09 1.36396452e+09 1.31468997e+09]\n",
      "[1.39260444e+09 1.41702727e+09 1.36475978e+09]\n",
      "[1.44311678e+09 1.46843755e+09 1.41532442e+09]\n",
      "[1.49530744e+09 1.52158305e+09 1.46480391e+09]\n",
      "[1.59313077e+09 1.62412187e+09 1.56587733e+09]\n",
      "[1.63563164e+09 1.66733590e+09 1.60507173e+09]\n",
      "[1.69316684e+09 1.72529632e+09 1.65887133e+09]\n",
      "[1.72979283e+09 1.76177975e+09 1.69400758e+09]\n",
      "[1.77893484e+09 1.81189160e+09 1.74268133e+09]\n",
      "[1.83826080e+09 1.87043088e+09 1.80159610e+09]\n",
      "[1.89926779e+09 1.93202829e+09 1.86115408e+09]\n",
      "[1.95374065e+09 1.98794639e+09 1.91437835e+09]\n",
      "[2.01090769e+09 2.04533093e+09 1.97128662e+09]\n",
      "[2.06687275e+09 2.10151966e+09 2.02724531e+09]\n",
      "[2.12001784e+09 2.15399174e+09 2.07907358e+09]\n",
      "[2.17315351e+09 2.20778522e+09 2.13086824e+09]\n",
      "[2.22771429e+09 2.26323631e+09 2.18464595e+09]\n",
      "[2.28089576e+09 2.31574814e+09 2.23651606e+09]\n",
      "[2.33941610e+09 2.37416034e+09 2.29381453e+09]\n",
      "[2.39894404e+09 2.43433704e+09 2.35188828e+09]\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_std(folder):\n",
    "    \"\"\"\n",
    "    folder: base folder where sampled_frames/ is located\n",
    "    this function is used to calculate the mean and std to normalize the image\n",
    "    \"\"\"\n",
    "    n_pixels = 0\n",
    "    mean_sum = np.zeros(3, dtype=np.float64)\n",
    "    sq_sum = np.zeros(3, dtype=np.float64)\n",
    "    video_folders = [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "    for vf in video_folders:\n",
    "        files = [os.path.join(vf, f) for f in os.listdir(vf)]\n",
    "        for img_path in files:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0 #Converts the image from BGR to RGB color format adn normalizes the value to be between 0 and 1\n",
    "            img = img.reshape(-1, 3) # we convert it into a 2d array basically flattening the image\n",
    "\n",
    "            mean_sum += img.sum(axis=0)\n",
    "            sq_sum += (img ** 2).sum(axis=0)\n",
    "            n_pixels += img.shape[0]\n",
    "        print(mean_sum)\n",
    "    mean = mean_sum / n_pixels\n",
    "    std = np.sqrt(sq_sum / n_pixels - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "mean, std = calculate_mean_std(\"sampled_frames\")\n",
    "print(\"mean:\", mean, \"std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499b4d92-6401-4ec4-a3dd-9d743dfbe5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir=\"sampled_frames\", seq_len=60,\n",
    "                 video_col='video_id', path_col='path', label_col='label',\n",
    "                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        \"\"\"\n",
    "        csv_file: CSV with columns : video_id, path, label\n",
    "        root_dir: folder where video frame folders are located\n",
    "        seq_len: number of frames per clip (pad/subsample if needed)\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.video_col = video_col\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "\n",
    "        self.video_paths = self.df[self.path_col].tolist()\n",
    "        self.labels = self.df[self.label_col].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def load_frames_from_video(self, video_folder):\n",
    "        \"\"\"Loads and returns all frames from one folder.\"\"\"\n",
    "        frames = []\n",
    "        folder_path = os.path.join(self.root_dir, video_folder)\n",
    "        folder_path = os.path.normpath(folder_path)  # normalize slashes to correct the path\n",
    "        files = sorted(os.listdir(folder_path))      # ensure correct frame order by sorting it\n",
    "\n",
    "        for f in files:\n",
    "            img_path = os.path.join(folder_path, f)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue  # skip missing/broken images\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(img)\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_folder = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        frames = self.load_frames_from_video(video_folder)\n",
    "\n",
    "        # apply transform\n",
    "        frames = [self.transform(frame) for frame in frames]\n",
    "        frames = torch.stack(frames)  # shape: (number_of_frames, 3, H, W)\n",
    "\n",
    "        return frames, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b3a7d9f-6088-4586-b5d3-a9de8cb1c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 60, 3, 224, 224]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = VideoDataset(csv_file=\"data_final.csv\", seq_len=60)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True) # batch_size of 2 for testing\n",
    "\n",
    "for frames, labels in loader:\n",
    "    print(frames.shape, labels) # returns (batch_size, number_of_frames, color_channels. height, width)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13e63f12-7b4b-4620-ac04-a5e4e08c7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=2, hidden_dim=128, num_layers=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        # 2D CNN to capture spacial data\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # input 3x224x224\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x112x112\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x56x56\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x28x28\n",
    "        )\n",
    "\n",
    "        self.feature_dim = 64 * 28 * 28  # each frame turned into a vector of 64*128*128 size\n",
    "        # lstm with hidden_dim = 128, consisting of 60 feature vectors\n",
    "        self.lstm = nn.LSTM(input_size=self.feature_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        # fully-connected layer for classification (it maps it to logits)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, C, H, W]\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.size()\n",
    "\n",
    "        # Merge batch and time for CNN\n",
    "        x = x.view(B*T, C, H, W)  # [B*T, 3, H, W] combining batch and sequence dimension to treat each frame individually\n",
    "        features = self.cnn(x)\n",
    "        features = features.view(B, T, -1)  # flatten to [B, T, feature_dim] for lstm\n",
    "\n",
    "        lstm_out, _ = self.lstm(features)  # [B, T, hidden_dim] # take the last hidden state only\n",
    "\n",
    "        # Use last time step for classification\n",
    "        last_out = lstm_out[:, -1, :]  # [B, hidden_dim]\n",
    "\n",
    "        out = self.fc(last_out)  # [B, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8ee8bef-d80d-4b7d-a7e6-b808bdfae702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "model = CNN_LSTM(num_classes=2)\n",
    "x = torch.randn(2, 60, 3, 224, 224)  # batch_size=2, seq_len=60\n",
    "out = model(x)\n",
    "print(out.shape)  # (batch_size, number_of_classes) basically a list of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a6fb2e-c507-4255-b3f1-c19813f30d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f35242-4736-42fd-a37f-7d295b6dbad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10] Train Loss: 0.6390 | Train Acc: 0.6667 | Val Loss: 0.5801 | Val Acc: 0.7246\n",
      "Epoch [2/10] Train Loss: 0.5741 | Train Acc: 0.7068 | Val Loss: 0.5477 | Val Acc: 0.7101\n",
      "Epoch [3/10] Train Loss: 0.5204 | Train Acc: 0.7377 | Val Loss: 0.5384 | Val Acc: 0.7536\n",
      "Epoch [4/10] Train Loss: 0.4865 | Train Acc: 0.7685 | Val Loss: 0.5522 | Val Acc: 0.7536\n",
      "Epoch [5/10] Train Loss: 0.4242 | Train Acc: 0.8025 | Val Loss: 0.5998 | Val Acc: 0.7391\n",
      "Epoch [6/10] Train Loss: 0.3662 | Train Acc: 0.8457 | Val Loss: 0.6235 | Val Acc: 0.7681\n",
      "Epoch [7/10] Train Loss: 0.3754 | Train Acc: 0.8241 | Val Loss: 0.5224 | Val Acc: 0.7101\n",
      "Epoch [8/10] Train Loss: 0.3033 | Train Acc: 0.8704 | Val Loss: 0.6540 | Val Acc: 0.7391\n",
      "Epoch [9/10] Train Loss: 0.2371 | Train Acc: 0.8858 | Val Loss: 0.6603 | Val Acc: 0.7246\n",
      "Epoch [10/10] Train Loss: 0.1958 | Train Acc: 0.9228 | Val Loss: 0.6459 | Val Acc: 0.7536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 60\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "\n",
    "# using nvidia gpu for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# creating the training and validation dataset\n",
    "train_dataset = VideoDataset(csv_file=\"splits/train.csv\", seq_len=seq_len)\n",
    "val_dataset = VideoDataset(csv_file=\"splits/val.csv\", seq_len=seq_len)\n",
    "# creating the loaders for training and validation dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# defining the model\n",
    "model = CNN_LSTM(num_classes=num_classes, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "# using adam optimizer for smoother and faster training\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# here we use cross-entropy loss as it is a binary classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for frames, labels in train_loader:\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #using the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping to prevent gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * frames.size(0)\n",
    "        # taking the label with greater logit\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # checking against the label\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    # averaging the loss and accuracy over the training loop\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in val_loader:\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * frames.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5115672-8924-4aa7-aa21-24a73aff96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTM(num_classes=2)\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_lstm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb6a56-c03e-47af-b66b-99a4dc5ece67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
